{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-26T07:08:21.554508Z","iopub.execute_input":"2022-07-26T07:08:21.554863Z","iopub.status.idle":"2022-07-26T07:08:21.582531Z","shell.execute_reply.started":"2022-07-26T07:08:21.554782Z","shell.execute_reply":"2022-07-26T07:08:21.581839Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/dennisdutchlangdataset/dataset.csv\")\n\ndf[\"title\"] = df.text.apply(lambda x: x.split(\"\\n\")[0].strip())\ndf[\"text\"] = df.text.apply(lambda x: \"\\n\".join(x.split(\"\\n\")[1:]).strip())","metadata":{"execution":{"iopub.status.busy":"2022-07-26T07:08:21.584185Z","iopub.execute_input":"2022-07-26T07:08:21.584506Z","iopub.status.idle":"2022-07-26T07:08:21.636786Z","shell.execute_reply.started":"2022-07-26T07:08:21.584466Z","shell.execute_reply":"2022-07-26T07:08:21.636170Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/LIAAD/yake -q","metadata":{"execution":{"iopub.status.busy":"2022-07-26T07:08:21.638239Z","iopub.execute_input":"2022-07-26T07:08:21.638905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yake\nfrom tqdm.notebook import tqdm\n\nall_keywords = []\nfor text in tqdm(df.text.tolist()):\n\n    kw_extractor = yake.KeywordExtractor(lan=\"nl\")\n    extracted = kw_extractor.extract_keywords(text)\n\n    keywords = []\n\n    for kw in extracted:\n        keywords.append(kw)\n        \n    all_keywords.append(keywords)\n    \ndf[\"keywords\"] = all_keywords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"yhavinga/gpt2-medium-dutch\"\n\nkeyword_number = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ntraining_texts = []\n\nfor record in df.to_dict(\"record\"):\n    \n    keywords = [i[0].strip(\",\") for i in record[\"keywords\"][:keyword_number]]\n    text = record[\"text\"]\n    text = re.sub(\"[\\n]+\", \"\\n\\n\", text)\n    title = record[\"title\"]\n    \n    training_text = title + \"\\n\" + \", \".join(keywords) + \"\\n\\n#######\\n\\n\" + text\n    training_text = tokenizer.decode(tokenizer.encode(training_text)[:1024])\n    \n    training_texts.append(training_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"training_text\"] = training_texts\n\ntrain = df.iloc[:295]\ntest = df.iloc[295:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\\n\\n\\n---\\n\\n\\n\".join(training_texts)\n\nf = open(\"train.txt\", \"w\", encoding=\"utf-8\")\nf.write(text)\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install simpletransformers -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\n\nfrom simpletransformers.language_generation import LanguageGenerationModel, LanguageGenerationArgs\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.language_modeling import LanguageModelingModel\n\n\ntrain_args = {\n    \"reprocess_input_data\": True,\n    \"num_train_epochs\": 2,\n    \"save_eval_checkpoints\": False,\n    \"block_size\": 200,\n    \"max_seq_length\": 200,\n    # \"save_model_every_epoch\": False,\n    \"mlm\": False,\n    \"learning_rate\": 1e-4,\n    \"train_batch_size\": 1,\n    \"gradient_accumulation_steps\":2,\n    \"dataset_type\": \"simple\",\n    \"logging_steps\": 100,\n    \"evaluate_during_training\": True,\n    \"evaluate_during_training_steps\": 3000,\n    \"evaluate_during_training_verbose\": True,\n    \"use_cached_eval_features\": True,\n    \"sliding_window\": True,\n    \"use_multiprocessing\": False,\n    \"evaluate_during_training\": False,\n    \"num_beams\": 2\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Find a way to resize tokenizer instead of using standard one\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n\nclass SimpleDataset(Dataset):\n    def __init__(\n        self,\n        tokenizer,\n        file_path,\n    ):\n\n        with open(file_path, encoding=\"utf-8\") as f:\n            lines = [\n                tokenizer.encode(\n                    ele,\n                )\n                for ele in f.read().split(\"\\n\\n\\n---\\n\\n\\n\")\n            ]\n\n        self.examples = lines\n\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)\n\n\n    \ndef load_and_cache_examples(self, file_path, verbose=None, evaluate=None, silent=None):\n    return SimpleDataset(self.tokenizer, file_path)\n\n\nLanguageModelingModel.load_and_cache_examples = load_and_cache_examples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LanguageModelingModel(\"gpt2\", model_name, args=train_args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nmodel.train_model(\n    \"train.txt\", \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom simpletransformers.language_generation import LanguageGenerationModel\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\nmodel = LanguageGenerationModel(\"gpt2\", \"outputs\", args={\"max_length\": 512, \"top_k\":50, \"num_return_sequences\":1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"written_texts = test.training_text.apply(lambda x: x.split(\"#######\")[0] + \"#######\").tolist()\ntexts = []\n\nfor written_text in written_texts:\n    texts.append(model.generate(written_texts[0], verbose=False)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"written_texts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_text = \"\"\nfor text in texts:\n    \n    splitted = text.split(\"#######\")\n    output_text += f\"Input: {splitted[0]}\\n----\\noutput: {splitted[1]} \\n\\n\\n ****************** \\n\\n\\n\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open(\"examples_01.txt\", \"w\", encoding=\"utf-8\")\nfile.write(output_text)\nfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}