{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217a5ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:32.048388Z",
     "iopub.status.busy": "2022-07-17T15:56:32.047730Z",
     "iopub.status.idle": "2022-07-17T15:56:32.072815Z",
     "shell.execute_reply": "2022-07-17T15:56:32.071490Z"
    },
    "papermill": {
     "duration": 0.045231,
     "end_time": "2022-07-17T15:56:32.077166",
     "exception": false,
     "start_time": "2022-07-17T15:56:32.031935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ia-hackathonn/SampleSubmission.csv\n",
      "/kaggle/input/ia-hackathonn/Train (2).csv\n",
      "/kaggle/input/ia-hackathonn/Test (4).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b4b93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:32.107808Z",
     "iopub.status.busy": "2022-07-17T15:56:32.105487Z",
     "iopub.status.idle": "2022-07-17T15:56:33.262062Z",
     "shell.execute_reply": "2022-07-17T15:56:33.260740Z"
    },
    "papermill": {
     "duration": 1.17399,
     "end_time": "2022-07-17T15:56:33.265040",
     "exception": false,
     "start_time": "2022-07-17T15:56:32.091050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6e9bf",
   "metadata": {
    "papermill": {
     "duration": 0.012887,
     "end_time": "2022-07-17T15:56:33.291288",
     "exception": false,
     "start_time": "2022-07-17T15:56:33.278401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92448d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:33.319760Z",
     "iopub.status.busy": "2022-07-17T15:56:33.319366Z",
     "iopub.status.idle": "2022-07-17T15:56:37.249949Z",
     "shell.execute_reply": "2022-07-17T15:56:37.248630Z"
    },
    "papermill": {
     "duration": 3.948771,
     "end_time": "2022-07-17T15:56:37.253238",
     "exception": false,
     "start_time": "2022-07-17T15:56:33.304467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/ia-hackathonn/Train (2).csv')\n",
    "df_test = pd.read_csv('../input/ia-hackathonn/Test (4).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac5fc0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.281230Z",
     "iopub.status.busy": "2022-07-17T15:56:37.280812Z",
     "iopub.status.idle": "2022-07-17T15:56:37.291682Z",
     "shell.execute_reply": "2022-07-17T15:56:37.290212Z"
    },
    "papermill": {
     "duration": 0.030106,
     "end_time": "2022-07-17T15:56:37.296416",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.266310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 1795)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676f0b08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.323691Z",
     "iopub.status.busy": "2022-07-17T15:56:37.323314Z",
     "iopub.status.idle": "2022-07-17T15:56:37.331444Z",
     "shell.execute_reply": "2022-07-17T15:56:37.330141Z"
    },
    "papermill": {
     "duration": 0.025242,
     "end_time": "2022-07-17T15:56:37.334574",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.309332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 1793)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764f748f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.362364Z",
     "iopub.status.busy": "2022-07-17T15:56:37.361294Z",
     "iopub.status.idle": "2022-07-17T15:56:37.400585Z",
     "shell.execute_reply": "2022-07-17T15:56:37.399124Z"
    },
    "papermill": {
     "duration": 0.056546,
     "end_time": "2022-07-17T15:56:37.403941",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.347395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>channel_0_timestep_0</th>\n",
       "      <th>channel_0_timestep_1</th>\n",
       "      <th>channel_0_timestep_2</th>\n",
       "      <th>channel_0_timestep_3</th>\n",
       "      <th>channel_0_timestep_4</th>\n",
       "      <th>channel_0_timestep_5</th>\n",
       "      <th>channel_0_timestep_6</th>\n",
       "      <th>channel_0_timestep_7</th>\n",
       "      <th>channel_0_timestep_8</th>\n",
       "      <th>...</th>\n",
       "      <th>channel_13_timestep_120</th>\n",
       "      <th>channel_13_timestep_121</th>\n",
       "      <th>channel_13_timestep_122</th>\n",
       "      <th>channel_13_timestep_123</th>\n",
       "      <th>channel_13_timestep_124</th>\n",
       "      <th>channel_13_timestep_125</th>\n",
       "      <th>channel_13_timestep_126</th>\n",
       "      <th>channel_13_timestep_127</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1826</td>\n",
       "      <td>0.666593</td>\n",
       "      <td>-0.127720</td>\n",
       "      <td>0.030188</td>\n",
       "      <td>0.872432</td>\n",
       "      <td>0.066428</td>\n",
       "      <td>-0.688929</td>\n",
       "      <td>0.351763</td>\n",
       "      <td>0.962038</td>\n",
       "      <td>1.171841</td>\n",
       "      <td>...</td>\n",
       "      <td>4.268692</td>\n",
       "      <td>4.296591</td>\n",
       "      <td>0.405264</td>\n",
       "      <td>-2.827268</td>\n",
       "      <td>-2.004783</td>\n",
       "      <td>1.276288</td>\n",
       "      <td>2.222049</td>\n",
       "      <td>0.158765</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1959</td>\n",
       "      <td>1.439036</td>\n",
       "      <td>2.939323</td>\n",
       "      <td>4.450541</td>\n",
       "      <td>4.889985</td>\n",
       "      <td>2.808758</td>\n",
       "      <td>0.174926</td>\n",
       "      <td>-0.264143</td>\n",
       "      <td>-0.265241</td>\n",
       "      <td>-2.430413</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.719914</td>\n",
       "      <td>1.040441</td>\n",
       "      <td>0.272578</td>\n",
       "      <td>0.103977</td>\n",
       "      <td>3.468313</td>\n",
       "      <td>6.880353</td>\n",
       "      <td>8.626653</td>\n",
       "      <td>9.982868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>688</td>\n",
       "      <td>-6.873479</td>\n",
       "      <td>2.620711</td>\n",
       "      <td>6.368775</td>\n",
       "      <td>-0.753867</td>\n",
       "      <td>0.504129</td>\n",
       "      <td>8.989532</td>\n",
       "      <td>6.353325</td>\n",
       "      <td>-0.452068</td>\n",
       "      <td>-1.600631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>-5.156834</td>\n",
       "      <td>-18.325062</td>\n",
       "      <td>2.098055</td>\n",
       "      <td>17.815147</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>-3.858117</td>\n",
       "      <td>17.348724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>765</td>\n",
       "      <td>7.343672</td>\n",
       "      <td>-1.043033</td>\n",
       "      <td>3.394326</td>\n",
       "      <td>12.075764</td>\n",
       "      <td>2.094330</td>\n",
       "      <td>-6.726448</td>\n",
       "      <td>-2.849046</td>\n",
       "      <td>-2.859351</td>\n",
       "      <td>-3.247879</td>\n",
       "      <td>...</td>\n",
       "      <td>11.696478</td>\n",
       "      <td>5.142235</td>\n",
       "      <td>-2.881666</td>\n",
       "      <td>-3.570459</td>\n",
       "      <td>-4.366352</td>\n",
       "      <td>-4.056002</td>\n",
       "      <td>-1.489292</td>\n",
       "      <td>-2.761847</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>8.988179</td>\n",
       "      <td>1.338421</td>\n",
       "      <td>-2.789729</td>\n",
       "      <td>-4.878788</td>\n",
       "      <td>-7.294619</td>\n",
       "      <td>-6.117245</td>\n",
       "      <td>-3.501986</td>\n",
       "      <td>-3.073579</td>\n",
       "      <td>0.061395</td>\n",
       "      <td>...</td>\n",
       "      <td>5.989770</td>\n",
       "      <td>5.809627</td>\n",
       "      <td>4.878367</td>\n",
       "      <td>3.895410</td>\n",
       "      <td>0.987760</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>-0.732414</td>\n",
       "      <td>-3.816425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1795 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  channel_0_timestep_0  channel_0_timestep_1  channel_0_timestep_2  \\\n",
       "0  1826              0.666593             -0.127720              0.030188   \n",
       "1  1959              1.439036              2.939323              4.450541   \n",
       "2   688             -6.873479              2.620711              6.368775   \n",
       "3   765              7.343672             -1.043033              3.394326   \n",
       "4   610              8.988179              1.338421             -2.789729   \n",
       "\n",
       "   channel_0_timestep_3  channel_0_timestep_4  channel_0_timestep_5  \\\n",
       "0              0.872432              0.066428             -0.688929   \n",
       "1              4.889985              2.808758              0.174926   \n",
       "2             -0.753867              0.504129              8.989532   \n",
       "3             12.075764              2.094330             -6.726448   \n",
       "4             -4.878788             -7.294619             -6.117245   \n",
       "\n",
       "   channel_0_timestep_6  channel_0_timestep_7  channel_0_timestep_8  ...  \\\n",
       "0              0.351763              0.962038              1.171841  ...   \n",
       "1             -0.264143             -0.265241             -2.430413  ...   \n",
       "2              6.353325             -0.452068             -1.600631  ...   \n",
       "3             -2.849046             -2.859351             -3.247879  ...   \n",
       "4             -3.501986             -3.073579              0.061395  ...   \n",
       "\n",
       "   channel_13_timestep_120  channel_13_timestep_121  channel_13_timestep_122  \\\n",
       "0                 4.268692                 4.296591                 0.405264   \n",
       "1                -2.719914                 1.040441                 0.272578   \n",
       "2                 0.051793                -5.156834               -18.325062   \n",
       "3                11.696478                 5.142235                -2.881666   \n",
       "4                 5.989770                 5.809627                 4.878367   \n",
       "\n",
       "   channel_13_timestep_123  channel_13_timestep_124  channel_13_timestep_125  \\\n",
       "0                -2.827268                -2.004783                 1.276288   \n",
       "1                 0.103977                 3.468313                 6.880353   \n",
       "2                 2.098055                17.815147                 0.043855   \n",
       "3                -3.570459                -4.366352                -4.056002   \n",
       "4                 3.895410                 0.987760                 0.012008   \n",
       "\n",
       "   channel_13_timestep_126  channel_13_timestep_127  valence  arousal  \n",
       "0                 2.222049                 0.158765      5.0      6.0  \n",
       "1                 8.626653                 9.982868      1.0      7.0  \n",
       "2                -3.858117                17.348724      1.0      6.0  \n",
       "3                -1.489292                -2.761847      2.0      8.0  \n",
       "4                -0.732414                -3.816425      1.0      7.0  \n",
       "\n",
       "[5 rows x 1795 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df94ce4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.432414Z",
     "iopub.status.busy": "2022-07-17T15:56:37.432037Z",
     "iopub.status.idle": "2022-07-17T15:56:37.442174Z",
     "shell.execute_reply": "2022-07-17T15:56:37.440511Z"
    },
    "papermill": {
     "duration": 0.027706,
     "end_time": "2022-07-17T15:56:37.444986",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.417280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "v=df_train[\"valence\"]\n",
    "a=df_train[\"arousal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b062020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.472978Z",
     "iopub.status.busy": "2022-07-17T15:56:37.472602Z",
     "iopub.status.idle": "2022-07-17T15:56:37.484548Z",
     "shell.execute_reply": "2022-07-17T15:56:37.483267Z"
    },
    "papermill": {
     "duration": 0.028912,
     "end_time": "2022-07-17T15:56:37.487289",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.458377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 7., 8., 1., 5., 3., 2., 4., 9.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc0a5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.515711Z",
     "iopub.status.busy": "2022-07-17T15:56:37.514508Z",
     "iopub.status.idle": "2022-07-17T15:56:37.523265Z",
     "shell.execute_reply": "2022-07-17T15:56:37.522035Z"
    },
    "papermill": {
     "duration": 0.025801,
     "end_time": "2022-07-17T15:56:37.526151",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.500350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 1., 2., 4., 3., 7.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b20d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.554216Z",
     "iopub.status.busy": "2022-07-17T15:56:37.553823Z",
     "iopub.status.idle": "2022-07-17T15:56:37.561681Z",
     "shell.execute_reply": "2022-07-17T15:56:37.560384Z"
    },
    "papermill": {
     "duration": 0.025027,
     "end_time": "2022-07-17T15:56:37.564297",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.539270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'channel_0_timestep_0', 'channel_0_timestep_1',\n",
       "       'channel_0_timestep_2', 'channel_0_timestep_3', 'channel_0_timestep_4',\n",
       "       'channel_0_timestep_5', 'channel_0_timestep_6', 'channel_0_timestep_7',\n",
       "       'channel_0_timestep_8',\n",
       "       ...\n",
       "       'channel_13_timestep_120', 'channel_13_timestep_121',\n",
       "       'channel_13_timestep_122', 'channel_13_timestep_123',\n",
       "       'channel_13_timestep_124', 'channel_13_timestep_125',\n",
       "       'channel_13_timestep_126', 'channel_13_timestep_127', 'valence',\n",
       "       'arousal'],\n",
       "      dtype='object', length=1795)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2f3394",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.593714Z",
     "iopub.status.busy": "2022-07-17T15:56:37.592514Z",
     "iopub.status.idle": "2022-07-17T15:56:37.723143Z",
     "shell.execute_reply": "2022-07-17T15:56:37.721574Z"
    },
    "papermill": {
     "duration": 0.149729,
     "end_time": "2022-07-17T15:56:37.727467",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.577738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2898 entries, 0 to 2897\n",
      "Columns: 1795 entries, ID to arousal\n",
      "dtypes: float64(1794), int64(1)\n",
      "memory usage: 39.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677d1dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.759493Z",
     "iopub.status.busy": "2022-07-17T15:56:37.758225Z",
     "iopub.status.idle": "2022-07-17T15:56:37.765764Z",
     "shell.execute_reply": "2022-07-17T15:56:37.764480Z"
    },
    "papermill": {
     "duration": 0.025904,
     "end_time": "2022-07-17T15:56:37.768639",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.742735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Id=df_test[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0a3d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.798032Z",
     "iopub.status.busy": "2022-07-17T15:56:37.797613Z",
     "iopub.status.idle": "2022-07-17T15:56:37.828450Z",
     "shell.execute_reply": "2022-07-17T15:56:37.826888Z"
    },
    "papermill": {
     "duration": 0.049137,
     "end_time": "2022-07-17T15:56:37.831531",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.782394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>channel_0_timestep_0</th>\n",
       "      <th>channel_0_timestep_1</th>\n",
       "      <th>channel_0_timestep_2</th>\n",
       "      <th>channel_0_timestep_3</th>\n",
       "      <th>channel_0_timestep_4</th>\n",
       "      <th>channel_0_timestep_5</th>\n",
       "      <th>channel_0_timestep_6</th>\n",
       "      <th>channel_0_timestep_7</th>\n",
       "      <th>channel_0_timestep_8</th>\n",
       "      <th>...</th>\n",
       "      <th>channel_13_timestep_118</th>\n",
       "      <th>channel_13_timestep_119</th>\n",
       "      <th>channel_13_timestep_120</th>\n",
       "      <th>channel_13_timestep_121</th>\n",
       "      <th>channel_13_timestep_122</th>\n",
       "      <th>channel_13_timestep_123</th>\n",
       "      <th>channel_13_timestep_124</th>\n",
       "      <th>channel_13_timestep_125</th>\n",
       "      <th>channel_13_timestep_126</th>\n",
       "      <th>channel_13_timestep_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2649</td>\n",
       "      <td>0.726739</td>\n",
       "      <td>1.326801</td>\n",
       "      <td>0.419562</td>\n",
       "      <td>0.362655</td>\n",
       "      <td>1.877844</td>\n",
       "      <td>2.177589</td>\n",
       "      <td>0.881530</td>\n",
       "      <td>-0.177988</td>\n",
       "      <td>-0.107699</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.538226</td>\n",
       "      <td>-6.132041</td>\n",
       "      <td>-4.516711</td>\n",
       "      <td>-4.155573</td>\n",
       "      <td>-6.451481</td>\n",
       "      <td>-5.906953</td>\n",
       "      <td>-1.217648</td>\n",
       "      <td>2.809138</td>\n",
       "      <td>5.387832</td>\n",
       "      <td>7.240309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897</td>\n",
       "      <td>-2.290174</td>\n",
       "      <td>-1.321119</td>\n",
       "      <td>-4.212674</td>\n",
       "      <td>-2.404299</td>\n",
       "      <td>3.280274</td>\n",
       "      <td>5.224883</td>\n",
       "      <td>5.204860</td>\n",
       "      <td>3.618460</td>\n",
       "      <td>-2.801359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414722</td>\n",
       "      <td>3.437341</td>\n",
       "      <td>2.709642</td>\n",
       "      <td>-4.540618</td>\n",
       "      <td>-8.604247</td>\n",
       "      <td>-8.877815</td>\n",
       "      <td>-5.269321</td>\n",
       "      <td>4.677503</td>\n",
       "      <td>11.745440</td>\n",
       "      <td>11.366855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3973</td>\n",
       "      <td>-0.403232</td>\n",
       "      <td>-1.047966</td>\n",
       "      <td>-0.414630</td>\n",
       "      <td>-1.139564</td>\n",
       "      <td>-3.021573</td>\n",
       "      <td>-1.477372</td>\n",
       "      <td>0.307533</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.410522</td>\n",
       "      <td>...</td>\n",
       "      <td>3.059209</td>\n",
       "      <td>0.482465</td>\n",
       "      <td>-0.488242</td>\n",
       "      <td>2.093967</td>\n",
       "      <td>0.955976</td>\n",
       "      <td>-1.962847</td>\n",
       "      <td>-0.305406</td>\n",
       "      <td>2.123710</td>\n",
       "      <td>2.303813</td>\n",
       "      <td>1.377429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3038</td>\n",
       "      <td>2.900014</td>\n",
       "      <td>0.531952</td>\n",
       "      <td>-1.543073</td>\n",
       "      <td>-2.262296</td>\n",
       "      <td>3.178448</td>\n",
       "      <td>5.404681</td>\n",
       "      <td>1.022172</td>\n",
       "      <td>1.415055</td>\n",
       "      <td>2.566809</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019735</td>\n",
       "      <td>1.108618</td>\n",
       "      <td>-1.554358</td>\n",
       "      <td>-6.510234</td>\n",
       "      <td>-8.960296</td>\n",
       "      <td>-3.087777</td>\n",
       "      <td>6.820467</td>\n",
       "      <td>12.740038</td>\n",
       "      <td>12.078416</td>\n",
       "      <td>2.254736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>-3.372348</td>\n",
       "      <td>-1.855444</td>\n",
       "      <td>4.717225</td>\n",
       "      <td>1.641804</td>\n",
       "      <td>-8.900145</td>\n",
       "      <td>-11.168015</td>\n",
       "      <td>-9.589520</td>\n",
       "      <td>-12.598769</td>\n",
       "      <td>-9.765525</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.035542</td>\n",
       "      <td>-12.719938</td>\n",
       "      <td>-9.132347</td>\n",
       "      <td>-10.269296</td>\n",
       "      <td>-6.274572</td>\n",
       "      <td>1.645681</td>\n",
       "      <td>-2.948617</td>\n",
       "      <td>-10.918031</td>\n",
       "      <td>-5.863556</td>\n",
       "      <td>-1.668832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1793 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  channel_0_timestep_0  channel_0_timestep_1  channel_0_timestep_2  \\\n",
       "0  2649              0.726739              1.326801              0.419562   \n",
       "1  1897             -2.290174             -1.321119             -4.212674   \n",
       "2  3973             -0.403232             -1.047966             -0.414630   \n",
       "3  3038              2.900014              0.531952             -1.543073   \n",
       "4   494             -3.372348             -1.855444              4.717225   \n",
       "\n",
       "   channel_0_timestep_3  channel_0_timestep_4  channel_0_timestep_5  \\\n",
       "0              0.362655              1.877844              2.177589   \n",
       "1             -2.404299              3.280274              5.224883   \n",
       "2             -1.139564             -3.021573             -1.477372   \n",
       "3             -2.262296              3.178448              5.404681   \n",
       "4              1.641804             -8.900145            -11.168015   \n",
       "\n",
       "   channel_0_timestep_6  channel_0_timestep_7  channel_0_timestep_8  ...  \\\n",
       "0              0.881530             -0.177988             -0.107699  ...   \n",
       "1              5.204860              3.618460             -2.801359  ...   \n",
       "2              0.307533              0.156880              0.410522  ...   \n",
       "3              1.022172              1.415055              2.566809  ...   \n",
       "4             -9.589520            -12.598769             -9.765525  ...   \n",
       "\n",
       "   channel_13_timestep_118  channel_13_timestep_119  channel_13_timestep_120  \\\n",
       "0                -4.538226                -6.132041                -4.516711   \n",
       "1                 0.414722                 3.437341                 2.709642   \n",
       "2                 3.059209                 0.482465                -0.488242   \n",
       "3                -1.019735                 1.108618                -1.554358   \n",
       "4               -12.035542               -12.719938                -9.132347   \n",
       "\n",
       "   channel_13_timestep_121  channel_13_timestep_122  channel_13_timestep_123  \\\n",
       "0                -4.155573                -6.451481                -5.906953   \n",
       "1                -4.540618                -8.604247                -8.877815   \n",
       "2                 2.093967                 0.955976                -1.962847   \n",
       "3                -6.510234                -8.960296                -3.087777   \n",
       "4               -10.269296                -6.274572                 1.645681   \n",
       "\n",
       "   channel_13_timestep_124  channel_13_timestep_125  channel_13_timestep_126  \\\n",
       "0                -1.217648                 2.809138                 5.387832   \n",
       "1                -5.269321                 4.677503                11.745440   \n",
       "2                -0.305406                 2.123710                 2.303813   \n",
       "3                 6.820467                12.740038                12.078416   \n",
       "4                -2.948617               -10.918031                -5.863556   \n",
       "\n",
       "   channel_13_timestep_127  \n",
       "0                 7.240309  \n",
       "1                11.366855  \n",
       "2                 1.377429  \n",
       "3                 2.254736  \n",
       "4                -1.668832  \n",
       "\n",
       "[5 rows x 1793 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e850c255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.862681Z",
     "iopub.status.busy": "2022-07-17T15:56:37.861498Z",
     "iopub.status.idle": "2022-07-17T15:56:37.898265Z",
     "shell.execute_reply": "2022-07-17T15:56:37.896915Z"
    },
    "papermill": {
     "duration": 0.0556,
     "end_time": "2022-07-17T15:56:37.901505",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.845905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.drop(\"ID\" , axis=1, inplace=True)\n",
    "df_test.drop(\"ID\" , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668010f1",
   "metadata": {
    "papermill": {
     "duration": 0.014013,
     "end_time": "2022-07-17T15:56:37.930260",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.916247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Processing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed5aa82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:37.961147Z",
     "iopub.status.busy": "2022-07-17T15:56:37.959832Z",
     "iopub.status.idle": "2022-07-17T15:56:37.987902Z",
     "shell.execute_reply": "2022-07-17T15:56:37.986572Z"
    },
    "papermill": {
     "duration": 0.046343,
     "end_time": "2022-07-17T15:56:37.990606",
     "exception": false,
     "start_time": "2022-07-17T15:56:37.944263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channel_0_timestep_0      0\n",
       "channel_10_timestep_65    0\n",
       "channel_9_timestep_51     0\n",
       "channel_9_timestep_50     0\n",
       "channel_9_timestep_49     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum().sort_values(ascending =False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ff216",
   "metadata": {
    "papermill": {
     "duration": 0.013957,
     "end_time": "2022-07-17T15:56:38.019216",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.005259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Treatment of left parameters having missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385c1ed",
   "metadata": {
    "papermill": {
     "duration": 0.013935,
     "end_time": "2022-07-17T15:56:38.047321",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.033386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8bf0694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.078438Z",
     "iopub.status.busy": "2022-07-17T15:56:38.076766Z",
     "iopub.status.idle": "2022-07-17T15:56:38.101695Z",
     "shell.execute_reply": "2022-07-17T15:56:38.100382Z"
    },
    "papermill": {
     "duration": 0.043524,
     "end_time": "2022-07-17T15:56:38.104869",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.061345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainn=df_train.drop(\"valence\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beed4fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.135036Z",
     "iopub.status.busy": "2022-07-17T15:56:38.134637Z",
     "iopub.status.idle": "2022-07-17T15:56:38.173917Z",
     "shell.execute_reply": "2022-07-17T15:56:38.172628Z"
    },
    "papermill": {
     "duration": 0.059563,
     "end_time": "2022-07-17T15:56:38.179034",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.119471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_0_timestep_0</th>\n",
       "      <th>channel_0_timestep_1</th>\n",
       "      <th>channel_0_timestep_2</th>\n",
       "      <th>channel_0_timestep_3</th>\n",
       "      <th>channel_0_timestep_4</th>\n",
       "      <th>channel_0_timestep_5</th>\n",
       "      <th>channel_0_timestep_6</th>\n",
       "      <th>channel_0_timestep_7</th>\n",
       "      <th>channel_0_timestep_8</th>\n",
       "      <th>channel_0_timestep_9</th>\n",
       "      <th>...</th>\n",
       "      <th>channel_13_timestep_119</th>\n",
       "      <th>channel_13_timestep_120</th>\n",
       "      <th>channel_13_timestep_121</th>\n",
       "      <th>channel_13_timestep_122</th>\n",
       "      <th>channel_13_timestep_123</th>\n",
       "      <th>channel_13_timestep_124</th>\n",
       "      <th>channel_13_timestep_125</th>\n",
       "      <th>channel_13_timestep_126</th>\n",
       "      <th>channel_13_timestep_127</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666593</td>\n",
       "      <td>-0.127720</td>\n",
       "      <td>0.030188</td>\n",
       "      <td>0.872432</td>\n",
       "      <td>0.066428</td>\n",
       "      <td>-0.688929</td>\n",
       "      <td>0.351763</td>\n",
       "      <td>0.962038</td>\n",
       "      <td>1.171841</td>\n",
       "      <td>2.323957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378793</td>\n",
       "      <td>4.268692</td>\n",
       "      <td>4.296591</td>\n",
       "      <td>0.405264</td>\n",
       "      <td>-2.827268</td>\n",
       "      <td>-2.004783</td>\n",
       "      <td>1.276288</td>\n",
       "      <td>2.222049</td>\n",
       "      <td>0.158765</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.439036</td>\n",
       "      <td>2.939323</td>\n",
       "      <td>4.450541</td>\n",
       "      <td>4.889985</td>\n",
       "      <td>2.808758</td>\n",
       "      <td>0.174926</td>\n",
       "      <td>-0.264143</td>\n",
       "      <td>-0.265241</td>\n",
       "      <td>-2.430413</td>\n",
       "      <td>-4.575553</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.864929</td>\n",
       "      <td>-2.719914</td>\n",
       "      <td>1.040441</td>\n",
       "      <td>0.272578</td>\n",
       "      <td>0.103977</td>\n",
       "      <td>3.468313</td>\n",
       "      <td>6.880353</td>\n",
       "      <td>8.626653</td>\n",
       "      <td>9.982868</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.873479</td>\n",
       "      <td>2.620711</td>\n",
       "      <td>6.368775</td>\n",
       "      <td>-0.753867</td>\n",
       "      <td>0.504129</td>\n",
       "      <td>8.989532</td>\n",
       "      <td>6.353325</td>\n",
       "      <td>-0.452068</td>\n",
       "      <td>-1.600631</td>\n",
       "      <td>-5.637474</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.872344</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>-5.156834</td>\n",
       "      <td>-18.325062</td>\n",
       "      <td>2.098055</td>\n",
       "      <td>17.815147</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>-3.858117</td>\n",
       "      <td>17.348724</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.343672</td>\n",
       "      <td>-1.043033</td>\n",
       "      <td>3.394326</td>\n",
       "      <td>12.075764</td>\n",
       "      <td>2.094330</td>\n",
       "      <td>-6.726448</td>\n",
       "      <td>-2.849046</td>\n",
       "      <td>-2.859351</td>\n",
       "      <td>-3.247879</td>\n",
       "      <td>3.040648</td>\n",
       "      <td>...</td>\n",
       "      <td>6.079260</td>\n",
       "      <td>11.696478</td>\n",
       "      <td>5.142235</td>\n",
       "      <td>-2.881666</td>\n",
       "      <td>-3.570459</td>\n",
       "      <td>-4.366352</td>\n",
       "      <td>-4.056002</td>\n",
       "      <td>-1.489292</td>\n",
       "      <td>-2.761847</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.988179</td>\n",
       "      <td>1.338421</td>\n",
       "      <td>-2.789729</td>\n",
       "      <td>-4.878788</td>\n",
       "      <td>-7.294619</td>\n",
       "      <td>-6.117245</td>\n",
       "      <td>-3.501986</td>\n",
       "      <td>-3.073579</td>\n",
       "      <td>0.061395</td>\n",
       "      <td>6.580954</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013592</td>\n",
       "      <td>5.989770</td>\n",
       "      <td>5.809627</td>\n",
       "      <td>4.878367</td>\n",
       "      <td>3.895410</td>\n",
       "      <td>0.987760</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>-0.732414</td>\n",
       "      <td>-3.816425</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>2.597523</td>\n",
       "      <td>-5.404371</td>\n",
       "      <td>-7.261963</td>\n",
       "      <td>-5.821558</td>\n",
       "      <td>-6.905499</td>\n",
       "      <td>-3.372167</td>\n",
       "      <td>-0.098528</td>\n",
       "      <td>-4.118333</td>\n",
       "      <td>-2.304121</td>\n",
       "      <td>2.807478</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.493681</td>\n",
       "      <td>-8.553452</td>\n",
       "      <td>-4.326558</td>\n",
       "      <td>-2.547905</td>\n",
       "      <td>1.256500</td>\n",
       "      <td>8.988491</td>\n",
       "      <td>12.065654</td>\n",
       "      <td>9.896533</td>\n",
       "      <td>7.962224</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>0.023456</td>\n",
       "      <td>-5.002498</td>\n",
       "      <td>-6.201032</td>\n",
       "      <td>-0.268211</td>\n",
       "      <td>1.768780</td>\n",
       "      <td>-2.178749</td>\n",
       "      <td>0.325151</td>\n",
       "      <td>7.422237</td>\n",
       "      <td>7.472040</td>\n",
       "      <td>4.527164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105220</td>\n",
       "      <td>-0.876480</td>\n",
       "      <td>1.165987</td>\n",
       "      <td>3.133366</td>\n",
       "      <td>2.900286</td>\n",
       "      <td>3.563294</td>\n",
       "      <td>3.955818</td>\n",
       "      <td>4.124920</td>\n",
       "      <td>7.065790</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>0.418006</td>\n",
       "      <td>0.489591</td>\n",
       "      <td>0.399380</td>\n",
       "      <td>0.076094</td>\n",
       "      <td>-0.080600</td>\n",
       "      <td>0.114078</td>\n",
       "      <td>0.445269</td>\n",
       "      <td>0.719440</td>\n",
       "      <td>0.730063</td>\n",
       "      <td>0.520684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971298</td>\n",
       "      <td>2.296222</td>\n",
       "      <td>2.376399</td>\n",
       "      <td>1.750293</td>\n",
       "      <td>0.803550</td>\n",
       "      <td>-2.585437</td>\n",
       "      <td>-4.914431</td>\n",
       "      <td>-2.477296</td>\n",
       "      <td>-0.020557</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>2.673160</td>\n",
       "      <td>1.955546</td>\n",
       "      <td>0.872103</td>\n",
       "      <td>-0.888802</td>\n",
       "      <td>-1.995294</td>\n",
       "      <td>-0.695374</td>\n",
       "      <td>1.180189</td>\n",
       "      <td>1.196407</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>-1.190031</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.766486</td>\n",
       "      <td>-6.545377</td>\n",
       "      <td>-3.103716</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>-0.295690</td>\n",
       "      <td>1.441595</td>\n",
       "      <td>10.695235</td>\n",
       "      <td>12.365301</td>\n",
       "      <td>-1.123650</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>-15.531085</td>\n",
       "      <td>-13.170703</td>\n",
       "      <td>-13.555800</td>\n",
       "      <td>-17.565762</td>\n",
       "      <td>-16.688934</td>\n",
       "      <td>-10.048054</td>\n",
       "      <td>-0.383483</td>\n",
       "      <td>14.014765</td>\n",
       "      <td>28.323889</td>\n",
       "      <td>38.613426</td>\n",
       "      <td>...</td>\n",
       "      <td>8.686493</td>\n",
       "      <td>7.239271</td>\n",
       "      <td>13.926379</td>\n",
       "      <td>20.090885</td>\n",
       "      <td>15.222136</td>\n",
       "      <td>8.454310</td>\n",
       "      <td>0.836808</td>\n",
       "      <td>-4.991370</td>\n",
       "      <td>0.566707</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2898 rows × 1793 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      channel_0_timestep_0  channel_0_timestep_1  channel_0_timestep_2  \\\n",
       "0                 0.666593             -0.127720              0.030188   \n",
       "1                 1.439036              2.939323              4.450541   \n",
       "2                -6.873479              2.620711              6.368775   \n",
       "3                 7.343672             -1.043033              3.394326   \n",
       "4                 8.988179              1.338421             -2.789729   \n",
       "...                    ...                   ...                   ...   \n",
       "2893              2.597523             -5.404371             -7.261963   \n",
       "2894              0.023456             -5.002498             -6.201032   \n",
       "2895              0.418006              0.489591              0.399380   \n",
       "2896              2.673160              1.955546              0.872103   \n",
       "2897            -15.531085            -13.170703            -13.555800   \n",
       "\n",
       "      channel_0_timestep_3  channel_0_timestep_4  channel_0_timestep_5  \\\n",
       "0                 0.872432              0.066428             -0.688929   \n",
       "1                 4.889985              2.808758              0.174926   \n",
       "2                -0.753867              0.504129              8.989532   \n",
       "3                12.075764              2.094330             -6.726448   \n",
       "4                -4.878788             -7.294619             -6.117245   \n",
       "...                    ...                   ...                   ...   \n",
       "2893             -5.821558             -6.905499             -3.372167   \n",
       "2894             -0.268211              1.768780             -2.178749   \n",
       "2895              0.076094             -0.080600              0.114078   \n",
       "2896             -0.888802             -1.995294             -0.695374   \n",
       "2897            -17.565762            -16.688934            -10.048054   \n",
       "\n",
       "      channel_0_timestep_6  channel_0_timestep_7  channel_0_timestep_8  \\\n",
       "0                 0.351763              0.962038              1.171841   \n",
       "1                -0.264143             -0.265241             -2.430413   \n",
       "2                 6.353325             -0.452068             -1.600631   \n",
       "3                -2.849046             -2.859351             -3.247879   \n",
       "4                -3.501986             -3.073579              0.061395   \n",
       "...                    ...                   ...                   ...   \n",
       "2893             -0.098528             -4.118333             -2.304121   \n",
       "2894              0.325151              7.422237              7.472040   \n",
       "2895              0.445269              0.719440              0.730063   \n",
       "2896              1.180189              1.196407              0.016508   \n",
       "2897             -0.383483             14.014765             28.323889   \n",
       "\n",
       "      channel_0_timestep_9  ...  channel_13_timestep_119  \\\n",
       "0                 2.323957  ...                 1.378793   \n",
       "1                -4.575553  ...                -6.864929   \n",
       "2                -5.637474  ...               -14.872344   \n",
       "3                 3.040648  ...                 6.079260   \n",
       "4                 6.580954  ...                 1.013592   \n",
       "...                    ...  ...                      ...   \n",
       "2893              2.807478  ...               -11.493681   \n",
       "2894              4.527164  ...                 0.105220   \n",
       "2895              0.520684  ...                 0.971298   \n",
       "2896             -1.190031  ...                -5.766486   \n",
       "2897             38.613426  ...                 8.686493   \n",
       "\n",
       "      channel_13_timestep_120  channel_13_timestep_121  \\\n",
       "0                    4.268692                 4.296591   \n",
       "1                   -2.719914                 1.040441   \n",
       "2                    0.051793                -5.156834   \n",
       "3                   11.696478                 5.142235   \n",
       "4                    5.989770                 5.809627   \n",
       "...                       ...                      ...   \n",
       "2893                -8.553452                -4.326558   \n",
       "2894                -0.876480                 1.165987   \n",
       "2895                 2.296222                 2.376399   \n",
       "2896                -6.545377                -3.103716   \n",
       "2897                 7.239271                13.926379   \n",
       "\n",
       "      channel_13_timestep_122  channel_13_timestep_123  \\\n",
       "0                    0.405264                -2.827268   \n",
       "1                    0.272578                 0.103977   \n",
       "2                  -18.325062                 2.098055   \n",
       "3                   -2.881666                -3.570459   \n",
       "4                    4.878367                 3.895410   \n",
       "...                       ...                      ...   \n",
       "2893                -2.547905                 1.256500   \n",
       "2894                 3.133366                 2.900286   \n",
       "2895                 1.750293                 0.803550   \n",
       "2896                 0.014929                -0.295690   \n",
       "2897                20.090885                15.222136   \n",
       "\n",
       "      channel_13_timestep_124  channel_13_timestep_125  \\\n",
       "0                   -2.004783                 1.276288   \n",
       "1                    3.468313                 6.880353   \n",
       "2                   17.815147                 0.043855   \n",
       "3                   -4.366352                -4.056002   \n",
       "4                    0.987760                 0.012008   \n",
       "...                       ...                      ...   \n",
       "2893                 8.988491                12.065654   \n",
       "2894                 3.563294                 3.955818   \n",
       "2895                -2.585437                -4.914431   \n",
       "2896                 1.441595                10.695235   \n",
       "2897                 8.454310                 0.836808   \n",
       "\n",
       "      channel_13_timestep_126  channel_13_timestep_127  arousal  \n",
       "0                    2.222049                 0.158765      6.0  \n",
       "1                    8.626653                 9.982868      7.0  \n",
       "2                   -3.858117                17.348724      6.0  \n",
       "3                   -1.489292                -2.761847      8.0  \n",
       "4                   -0.732414                -3.816425      7.0  \n",
       "...                       ...                      ...      ...  \n",
       "2893                 9.896533                 7.962224      3.0  \n",
       "2894                 4.124920                 7.065790      8.0  \n",
       "2895                -2.477296                -0.020557      5.0  \n",
       "2896                12.365301                -1.123650      7.0  \n",
       "2897                -4.991370                 0.566707      7.0  \n",
       "\n",
       "[2898 rows x 1793 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76600ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.211425Z",
     "iopub.status.busy": "2022-07-17T15:56:38.210966Z",
     "iopub.status.idle": "2022-07-17T15:56:38.237368Z",
     "shell.execute_reply": "2022-07-17T15:56:38.236048Z"
    },
    "papermill": {
     "duration": 0.045637,
     "end_time": "2022-07-17T15:56:38.240524",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.194887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainn=trainn.drop(\"arousal\" , axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc6cbb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.273392Z",
     "iopub.status.busy": "2022-07-17T15:56:38.272150Z",
     "iopub.status.idle": "2022-07-17T15:56:38.280833Z",
     "shell.execute_reply": "2022-07-17T15:56:38.279329Z"
    },
    "papermill": {
     "duration": 0.027898,
     "end_time": "2022-07-17T15:56:38.283706",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.255808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 1792)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8060ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.329414Z",
     "iopub.status.busy": "2022-07-17T15:56:38.328968Z",
     "iopub.status.idle": "2022-07-17T15:56:38.361517Z",
     "shell.execute_reply": "2022-07-17T15:56:38.360168Z"
    },
    "papermill": {
     "duration": 0.055455,
     "end_time": "2022-07-17T15:56:38.364950",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.309495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array(trainn)\n",
    "y_train = np.array(df_train[[\"valence\",\"arousal\"]])\n",
    "X_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d987c5b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.397218Z",
     "iopub.status.busy": "2022-07-17T15:56:38.396798Z",
     "iopub.status.idle": "2022-07-17T15:56:38.403740Z",
     "shell.execute_reply": "2022-07-17T15:56:38.402303Z"
    },
    "papermill": {
     "duration": 0.027254,
     "end_time": "2022-07-17T15:56:38.407590",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.380336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (2898, 1792) \n",
      "Shape of y_test (2898, 2)\n",
      "Shape of X_test (1242, 1792)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train {} \\nShape of y_test {}\\nShape of X_test {}'.format(X_train.shape,y_train.shape,X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8409f6",
   "metadata": {
    "papermill": {
     "duration": 0.014733,
     "end_time": "2022-07-17T15:56:38.438897",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.424164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ed1b1a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.473532Z",
     "iopub.status.busy": "2022-07-17T15:56:38.471528Z",
     "iopub.status.idle": "2022-07-17T15:56:38.687045Z",
     "shell.execute_reply": "2022-07-17T15:56:38.685695Z"
    },
    "papermill": {
     "duration": 0.23528,
     "end_time": "2022-07-17T15:56:38.690024",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.454744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c3b6444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.721976Z",
     "iopub.status.busy": "2022-07-17T15:56:38.721575Z",
     "iopub.status.idle": "2022-07-17T15:56:38.731103Z",
     "shell.execute_reply": "2022-07-17T15:56:38.729620Z"
    },
    "papermill": {
     "duration": 0.028148,
     "end_time": "2022-07-17T15:56:38.733589",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.705441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 6.],\n",
       "       [1., 7.],\n",
       "       [1., 6.],\n",
       "       ...,\n",
       "       [5., 5.],\n",
       "       [4., 7.],\n",
       "       [1., 7.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1147b6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.765367Z",
     "iopub.status.busy": "2022-07-17T15:56:38.764211Z",
     "iopub.status.idle": "2022-07-17T15:56:38.771413Z",
     "shell.execute_reply": "2022-07-17T15:56:38.770284Z"
    },
    "papermill": {
     "duration": 0.025577,
     "end_time": "2022-07-17T15:56:38.774068",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.748491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = mms.fit_transform(y_train.reshape(-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7b0a43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.808280Z",
     "iopub.status.busy": "2022-07-17T15:56:38.807114Z",
     "iopub.status.idle": "2022-07-17T15:56:38.815944Z",
     "shell.execute_reply": "2022-07-17T15:56:38.814651Z"
    },
    "papermill": {
     "duration": 0.028785,
     "end_time": "2022-07-17T15:56:38.818685",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.789900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.625     ],\n",
       "       [0.        , 0.75      ],\n",
       "       [0.        , 0.625     ],\n",
       "       ...,\n",
       "       [0.66666667, 0.5       ],\n",
       "       [0.5       , 0.75      ],\n",
       "       [0.        , 0.75      ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e46c1f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:38.851234Z",
     "iopub.status.busy": "2022-07-17T15:56:38.850784Z",
     "iopub.status.idle": "2022-07-17T15:56:45.506398Z",
     "shell.execute_reply": "2022-07-17T15:56:45.504836Z"
    },
    "papermill": {
     "duration": 6.675562,
     "end_time": "2022-07-17T15:56:45.509741",
     "exception": false,
     "start_time": "2022-07-17T15:56:38.834179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cabe74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:45.547297Z",
     "iopub.status.busy": "2022-07-17T15:56:45.546391Z",
     "iopub.status.idle": "2022-07-17T15:56:49.498255Z",
     "shell.execute_reply": "2022-07-17T15:56:49.496903Z"
    },
    "papermill": {
     "duration": 3.975437,
     "end_time": "2022-07-17T15:56:49.502643",
     "exception": false,
     "start_time": "2022-07-17T15:56:45.527206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 15:56:45.654165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:45.781092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:45.782358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:45.784242: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-17 15:56:45.784783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:45.786166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:45.787457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:49.127336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:49.128501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:49.129549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:56:49.130577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(Dense(units=512,activation='relu',kernel_initializer='uniform'))\n",
    "regressor.add(Dropout(0.5))\n",
    "regressor.add(Dense(units=256,activation='relu',kernel_initializer='uniform'))\n",
    "regressor.add(Dropout(0.25))\n",
    "regressor.add(Dense(units=128,activation='relu',kernel_initializer='uniform'))\n",
    "regressor.add(Dropout(0.25))\n",
    "regressor.add(Dense(units=64,activation='relu',kernel_initializer='uniform'))\n",
    "regressor.add(Dropout(0.25))\n",
    "regressor.add(Dense(units=2,activation='relu',kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4282060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:49.537277Z",
     "iopub.status.busy": "2022-07-17T15:56:49.536805Z",
     "iopub.status.idle": "2022-07-17T15:56:49.561350Z",
     "shell.execute_reply": "2022-07-17T15:56:49.560063Z"
    },
    "papermill": {
     "duration": 0.044499,
     "end_time": "2022-07-17T15:56:49.564520",
     "exception": false,
     "start_time": "2022-07-17T15:56:49.520021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam',loss='mean_absolute_error',metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c99922d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-07-17T15:56:49.599190Z",
     "iopub.status.busy": "2022-07-17T15:56:49.598750Z",
     "iopub.status.idle": "2022-07-17T15:58:12.487479Z",
     "shell.execute_reply": "2022-07-17T15:58:12.486109Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 83.028803,
     "end_time": "2022-07-17T15:58:12.609451",
     "exception": false,
     "start_time": "2022-07-17T15:56:49.580648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 15:56:49.753581: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "58/58 [==============================] - 2s 3ms/step - loss: 0.2846 - mean_absolute_error: 0.2846\n",
      "Epoch 2/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2647 - mean_absolute_error: 0.2647\n",
      "Epoch 3/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2605 - mean_absolute_error: 0.2605\n",
      "Epoch 4/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2620 - mean_absolute_error: 0.2620\n",
      "Epoch 5/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2608 - mean_absolute_error: 0.2608\n",
      "Epoch 6/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2582 - mean_absolute_error: 0.2582\n",
      "Epoch 7/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2554 - mean_absolute_error: 0.2554\n",
      "Epoch 8/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2579 - mean_absolute_error: 0.2579\n",
      "Epoch 9/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2536 - mean_absolute_error: 0.2536\n",
      "Epoch 10/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2550 - mean_absolute_error: 0.2550\n",
      "Epoch 11/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2522 - mean_absolute_error: 0.2522\n",
      "Epoch 12/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2523 - mean_absolute_error: 0.2523\n",
      "Epoch 13/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2525 - mean_absolute_error: 0.2525\n",
      "Epoch 14/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2515 - mean_absolute_error: 0.2515\n",
      "Epoch 15/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2496 - mean_absolute_error: 0.2496\n",
      "Epoch 16/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2487 - mean_absolute_error: 0.2487\n",
      "Epoch 17/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2478 - mean_absolute_error: 0.2478\n",
      "Epoch 18/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2490 - mean_absolute_error: 0.2490\n",
      "Epoch 19/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2471 - mean_absolute_error: 0.2471\n",
      "Epoch 20/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2473 - mean_absolute_error: 0.2473\n",
      "Epoch 21/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2466 - mean_absolute_error: 0.2466\n",
      "Epoch 22/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2469 - mean_absolute_error: 0.2469\n",
      "Epoch 23/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2463 - mean_absolute_error: 0.2463\n",
      "Epoch 24/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2464 - mean_absolute_error: 0.2464\n",
      "Epoch 25/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2464 - mean_absolute_error: 0.2464\n",
      "Epoch 26/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2468 - mean_absolute_error: 0.2468\n",
      "Epoch 27/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 28/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 29/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2459 - mean_absolute_error: 0.2459\n",
      "Epoch 30/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2463 - mean_absolute_error: 0.2463\n",
      "Epoch 31/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2461 - mean_absolute_error: 0.2461\n",
      "Epoch 32/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 33/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2460 - mean_absolute_error: 0.2460\n",
      "Epoch 34/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2462 - mean_absolute_error: 0.2462\n",
      "Epoch 35/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 36/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2462 - mean_absolute_error: 0.2462\n",
      "Epoch 37/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2461 - mean_absolute_error: 0.2461\n",
      "Epoch 38/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 39/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2459 - mean_absolute_error: 0.2459\n",
      "Epoch 40/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 41/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2459 - mean_absolute_error: 0.2459\n",
      "Epoch 42/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 43/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2457 - mean_absolute_error: 0.2457\n",
      "Epoch 44/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2457 - mean_absolute_error: 0.2457\n",
      "Epoch 45/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 46/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 47/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 48/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 49/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 50/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 51/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 52/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 53/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 54/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2457 - mean_absolute_error: 0.2457\n",
      "Epoch 55/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 56/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 57/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 58/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2458 - mean_absolute_error: 0.2458\n",
      "Epoch 59/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 60/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 61/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 62/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 63/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 64/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 65/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 66/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 67/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 68/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 69/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 70/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 71/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 72/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 73/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 74/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 75/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 76/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 77/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 78/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 79/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 80/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 81/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 82/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 83/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 84/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 85/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 86/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 87/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 88/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 89/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 90/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 91/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 92/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 93/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 94/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 95/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 96/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 97/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 98/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 99/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 100/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 101/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 102/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 103/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 104/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 105/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 106/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 107/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 108/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 109/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 110/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 111/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 112/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 113/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 114/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 115/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 116/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 117/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 118/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 119/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 120/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 121/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 122/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 123/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 124/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 125/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 126/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 127/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 128/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 129/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 130/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 131/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 132/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 133/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2441 - mean_absolute_error: 0.2441\n",
      "Epoch 134/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 135/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 136/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 137/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 138/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 139/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 140/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 141/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 142/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 143/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 144/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 145/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 146/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 147/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2454 - mean_absolute_error: 0.2454\n",
      "Epoch 148/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 149/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 150/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 151/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 152/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 153/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 154/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 155/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 156/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 157/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 158/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 159/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 160/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 161/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 162/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 163/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 164/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 165/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 166/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 167/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 168/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 169/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 170/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 171/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 172/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 173/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 174/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 175/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 176/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 177/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2452 - mean_absolute_error: 0.2452\n",
      "Epoch 178/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 179/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 180/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 181/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 182/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 183/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 184/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 185/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 186/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 187/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 188/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 189/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2442 - mean_absolute_error: 0.2442\n",
      "Epoch 190/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 191/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2453 - mean_absolute_error: 0.2453\n",
      "Epoch 192/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 193/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 194/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 195/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 196/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2451 - mean_absolute_error: 0.2451\n",
      "Epoch 197/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 198/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 199/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 200/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 201/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 202/250\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 0.2456 - mean_absolute_error: 0.2456\n",
      "Epoch 203/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 204/250\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 205/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 206/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 207/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 208/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 209/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 210/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 211/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 212/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 213/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 214/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 215/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 216/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 217/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 218/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 219/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 220/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 221/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 222/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 223/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 224/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 225/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 226/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2450 - mean_absolute_error: 0.2450\n",
      "Epoch 227/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 228/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 229/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 230/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 231/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 232/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 233/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 234/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 235/250\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 236/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 237/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 238/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 239/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2448 - mean_absolute_error: 0.2448\n",
      "Epoch 240/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 241/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2444 - mean_absolute_error: 0.2444\n",
      "Epoch 242/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 243/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2449 - mean_absolute_error: 0.2449\n",
      "Epoch 244/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2445 - mean_absolute_error: 0.2445\n",
      "Epoch 245/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 246/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 247/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 248/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2447 - mean_absolute_error: 0.2447\n",
      "Epoch 249/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n",
      "Epoch 250/250\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 0.2446 - mean_absolute_error: 0.2446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83c1c31690>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train,epochs=250,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05935b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:12.853056Z",
     "iopub.status.busy": "2022-07-17T15:58:12.852626Z",
     "iopub.status.idle": "2022-07-17T15:58:13.111384Z",
     "shell.execute_reply": "2022-07-17T15:58:13.110116Z"
    },
    "papermill": {
     "duration": 0.385408,
     "end_time": "2022-07-17T15:58:13.114260",
     "exception": false,
     "start_time": "2022-07-17T15:58:12.728852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = regressor.history.history\n",
    "losses = np.array(pd.DataFrame(losses))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79d61b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:13.359904Z",
     "iopub.status.busy": "2022-07-17T15:58:13.359382Z",
     "iopub.status.idle": "2022-07-17T15:58:13.568380Z",
     "shell.execute_reply": "2022-07-17T15:58:13.567059Z"
    },
    "papermill": {
     "duration": 0.33509,
     "end_time": "2022-07-17T15:58:13.571506",
     "exception": false,
     "start_time": "2022-07-17T15:58:13.236416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test) \n",
    "y_pred_original = mms.inverse_transform(y_pred.reshape(-1,2))\n",
    "y_pred_original = y_pred_original.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eecc4a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:13.821263Z",
     "iopub.status.busy": "2022-07-17T15:58:13.819050Z",
     "iopub.status.idle": "2022-07-17T15:58:13.829483Z",
     "shell.execute_reply": "2022-07-17T15:58:13.828209Z"
    },
    "papermill": {
     "duration": 0.13668,
     "end_time": "2022-07-17T15:58:13.832091",
     "exception": false,
     "start_time": "2022-07-17T15:58:13.695411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0042619705200195, 6.718179702758789]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2a4b843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:14.077829Z",
     "iopub.status.busy": "2022-07-17T15:58:14.076571Z",
     "iopub.status.idle": "2022-07-17T15:58:14.083531Z",
     "shell.execute_reply": "2022-07-17T15:58:14.082206Z"
    },
    "papermill": {
     "duration": 0.132558,
     "end_time": "2022-07-17T15:58:14.086571",
     "exception": false,
     "start_time": "2022-07-17T15:58:13.954013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=[i[0] for i in y_pred_original]\n",
    "y=[i[1] for i in y_pred_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca4291ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:14.330130Z",
     "iopub.status.busy": "2022-07-17T15:58:14.329701Z",
     "iopub.status.idle": "2022-07-17T15:58:14.337751Z",
     "shell.execute_reply": "2022-07-17T15:58:14.336255Z"
    },
    "papermill": {
     "duration": 0.134693,
     "end_time": "2022-07-17T15:58:14.340672",
     "exception": false,
     "start_time": "2022-07-17T15:58:14.205979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"ID\":Id,\"x\":x,\"y\":y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e11bcb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:14.645543Z",
     "iopub.status.busy": "2022-07-17T15:58:14.644846Z",
     "iopub.status.idle": "2022-07-17T15:58:14.661216Z",
     "shell.execute_reply": "2022-07-17T15:58:14.659957Z"
    },
    "papermill": {
     "duration": 0.200794,
     "end_time": "2022-07-17T15:58:14.663769",
     "exception": false,
     "start_time": "2022-07-17T15:58:14.462975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub1=df.to_csv(\"sub7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfce55f",
   "metadata": {
    "papermill": {
     "duration": 0.120166,
     "end_time": "2022-07-17T15:58:14.903755",
     "exception": false,
     "start_time": "2022-07-17T15:58:14.783589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93675924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:15.149440Z",
     "iopub.status.busy": "2022-07-17T15:58:15.148276Z",
     "iopub.status.idle": "2022-07-17T15:58:15.193346Z",
     "shell.execute_reply": "2022-07-17T15:58:15.192101Z"
    },
    "papermill": {
     "duration": 0.171863,
     "end_time": "2022-07-17T15:58:15.196551",
     "exception": false,
     "start_time": "2022-07-17T15:58:15.024688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(512, kernel_initializer='uniform', activation='relu'))\n",
    "NN_model.add(Dropout(0.5))\n",
    "NN_model.add(Dense(512, kernel_initializer='uniform', activation='relu'))\n",
    "NN_model.add(Dropout(0.5))\n",
    "NN_model.add(Dense(512, kernel_initializer='uniform', activation='relu'))\n",
    "NN_model.add(Dropout(0.5))\n",
    "NN_model.add(Dense(512, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dropout(0.5))\n",
    "NN_model.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dropout(0.25))\n",
    "NN_model.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dropout(0.2))\n",
    "NN_model.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dropout(0.25))\n",
    "NN_model.add(Dense(128, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dropout(0.25))\n",
    "NN_model.add(Dense(128, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dropout(0.25))\n",
    "NN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\n",
    "NN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\n",
    "              \n",
    "              \n",
    "    # The Output Layer :\n",
    "NN_model.add(Dense(2, kernel_initializer='uniform',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ce3aeb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:15.439769Z",
     "iopub.status.busy": "2022-07-17T15:58:15.439314Z",
     "iopub.status.idle": "2022-07-17T15:58:15.446127Z",
     "shell.execute_reply": "2022-07-17T15:58:15.444684Z"
    },
    "papermill": {
     "duration": 0.132756,
     "end_time": "2022-07-17T15:58:15.448937",
     "exception": false,
     "start_time": "2022-07-17T15:58:15.316181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "630f69f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:58:15.692552Z",
     "iopub.status.busy": "2022-07-17T15:58:15.692144Z",
     "iopub.status.idle": "2022-07-17T15:59:38.828669Z",
     "shell.execute_reply": "2022-07-17T15:59:38.827148Z"
    },
    "papermill": {
     "duration": 83.261906,
     "end_time": "2022-07-17T15:59:38.831406",
     "exception": false,
     "start_time": "2022-07-17T15:58:15.569500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "50/50 [==============================] - 2s 10ms/step - loss: 0.3299 - mean_absolute_error: 0.3299 - val_loss: 0.2571 - val_mean_absolute_error: 0.2571\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25713, saving model to Weights-001--0.25713.hdf5\n",
      "Epoch 2/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2661 - mean_absolute_error: 0.2661 - val_loss: 0.2582 - val_mean_absolute_error: 0.2582\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25713\n",
      "Epoch 3/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2647 - mean_absolute_error: 0.2647 - val_loss: 0.2535 - val_mean_absolute_error: 0.2535\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25713 to 0.25355, saving model to Weights-003--0.25355.hdf5\n",
      "Epoch 4/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2545 - mean_absolute_error: 0.2545 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25355 to 0.25212, saving model to Weights-004--0.25212.hdf5\n",
      "Epoch 5/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2498 - mean_absolute_error: 0.2498 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25212 to 0.24729, saving model to Weights-005--0.24729.hdf5\n",
      "Epoch 6/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2479 - mean_absolute_error: 0.2479 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24729\n",
      "Epoch 7/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2460 - mean_absolute_error: 0.2460 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24729 to 0.24669, saving model to Weights-007--0.24669.hdf5\n",
      "Epoch 8/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2457 - mean_absolute_error: 0.2457 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24669\n",
      "Epoch 9/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24669\n",
      "Epoch 10/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2451 - mean_absolute_error: 0.2451 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.24669 to 0.24641, saving model to Weights-010--0.24641.hdf5\n",
      "Epoch 11/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24641\n",
      "Epoch 12/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24641\n",
      "Epoch 13/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24641\n",
      "Epoch 14/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.24641\n",
      "Epoch 15/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.24641\n",
      "Epoch 16/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2487 - val_mean_absolute_error: 0.2487\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.24641\n",
      "Epoch 17/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.24641\n",
      "Epoch 18/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.24641\n",
      "Epoch 19/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.24641\n",
      "Epoch 20/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.24641 to 0.24613, saving model to Weights-020--0.24613.hdf5\n",
      "Epoch 21/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2455 - mean_absolute_error: 0.2455 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.24613\n",
      "Epoch 22/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2452 - mean_absolute_error: 0.2452 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.24613\n",
      "Epoch 23/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.24613\n",
      "Epoch 24/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.24613\n",
      "Epoch 25/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2452 - mean_absolute_error: 0.2452 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.24613\n",
      "Epoch 26/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.24613\n",
      "Epoch 27/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.24613\n",
      "Epoch 28/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24613\n",
      "Epoch 29/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.24613\n",
      "Epoch 30/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24613\n",
      "Epoch 31/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24613\n",
      "Epoch 32/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24613\n",
      "Epoch 33/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24613\n",
      "Epoch 34/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24613\n",
      "Epoch 35/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24613\n",
      "Epoch 36/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24613\n",
      "Epoch 37/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24613 to 0.24608, saving model to Weights-037--0.24608.hdf5\n",
      "Epoch 38/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24608\n",
      "Epoch 39/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24608\n",
      "Epoch 40/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.24608\n",
      "Epoch 41/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.24608\n",
      "Epoch 42/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24608\n",
      "Epoch 43/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24608\n",
      "Epoch 44/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.24608\n",
      "Epoch 45/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.24608\n",
      "Epoch 46/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.24608\n",
      "Epoch 47/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.24608\n",
      "Epoch 48/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.24608\n",
      "Epoch 49/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.24608\n",
      "Epoch 50/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.24608\n",
      "Epoch 51/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.24608\n",
      "Epoch 52/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.24608\n",
      "Epoch 53/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.24608\n",
      "Epoch 54/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.24608\n",
      "Epoch 55/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.24608\n",
      "Epoch 56/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.24608\n",
      "Epoch 57/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24608\n",
      "Epoch 58/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24608\n",
      "Epoch 59/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24608\n",
      "Epoch 60/250\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.24608 to 0.24597, saving model to Weights-060--0.24597.hdf5\n",
      "Epoch 61/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2451 - mean_absolute_error: 0.2451 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24597\n",
      "Epoch 62/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24597\n",
      "Epoch 63/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24597\n",
      "Epoch 64/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.24597\n",
      "Epoch 65/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.24597\n",
      "Epoch 66/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.24597\n",
      "Epoch 67/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.24597\n",
      "Epoch 68/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.24597\n",
      "Epoch 69/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24597\n",
      "Epoch 70/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24597\n",
      "Epoch 71/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24597\n",
      "Epoch 72/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24597\n",
      "Epoch 73/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24597\n",
      "Epoch 74/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24597\n",
      "Epoch 75/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24597\n",
      "Epoch 76/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.24597 to 0.24583, saving model to Weights-076--0.24583.hdf5\n",
      "Epoch 77/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24583\n",
      "Epoch 78/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24583\n",
      "Epoch 79/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24583\n",
      "Epoch 80/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24583\n",
      "Epoch 81/250\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24583\n",
      "Epoch 82/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24583\n",
      "Epoch 83/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.24583\n",
      "Epoch 84/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24583\n",
      "Epoch 85/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24583\n",
      "Epoch 86/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24583\n",
      "Epoch 87/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24583\n",
      "Epoch 88/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24583\n",
      "Epoch 89/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24583\n",
      "Epoch 90/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24583\n",
      "Epoch 91/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24583\n",
      "Epoch 92/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24583\n",
      "Epoch 93/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24583\n",
      "Epoch 94/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.24583\n",
      "Epoch 95/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24583\n",
      "Epoch 96/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24583\n",
      "Epoch 97/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24583\n",
      "Epoch 98/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24583\n",
      "Epoch 99/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24583\n",
      "Epoch 100/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.24583\n",
      "Epoch 101/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.24583\n",
      "Epoch 102/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.24583\n",
      "Epoch 103/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.24583\n",
      "Epoch 104/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24583\n",
      "Epoch 105/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.24583\n",
      "Epoch 106/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.24583\n",
      "Epoch 107/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24583\n",
      "Epoch 108/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24583\n",
      "Epoch 109/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24583\n",
      "Epoch 110/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24583\n",
      "Epoch 111/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24583\n",
      "Epoch 112/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24583\n",
      "Epoch 113/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24583\n",
      "Epoch 114/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24583\n",
      "Epoch 115/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24583\n",
      "Epoch 116/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24583\n",
      "Epoch 117/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.24583\n",
      "Epoch 118/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.24583\n",
      "Epoch 119/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.24583\n",
      "Epoch 120/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.24583\n",
      "Epoch 121/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.24583\n",
      "Epoch 122/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.24583\n",
      "Epoch 123/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.24583\n",
      "Epoch 124/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.24583\n",
      "Epoch 125/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.24583\n",
      "Epoch 126/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.24583\n",
      "Epoch 127/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.24583\n",
      "Epoch 128/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.24583\n",
      "Epoch 129/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.24583\n",
      "Epoch 130/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.24583\n",
      "Epoch 131/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.24583\n",
      "Epoch 132/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.24583\n",
      "Epoch 133/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.24583\n",
      "Epoch 134/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.24583\n",
      "Epoch 135/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.24583\n",
      "Epoch 136/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.24583\n",
      "Epoch 137/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.24583\n",
      "Epoch 138/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.24583\n",
      "Epoch 139/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.24583\n",
      "Epoch 140/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.24583\n",
      "Epoch 141/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.24583\n",
      "Epoch 142/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.24583\n",
      "Epoch 143/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.24583\n",
      "Epoch 144/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.24583\n",
      "Epoch 145/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.24583\n",
      "Epoch 146/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.24583\n",
      "Epoch 147/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.24583\n",
      "Epoch 148/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2453 - mean_absolute_error: 0.2453 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.24583\n",
      "Epoch 149/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.24583\n",
      "Epoch 150/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.24583\n",
      "Epoch 151/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.24583\n",
      "Epoch 152/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.24583\n",
      "Epoch 153/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.24583\n",
      "Epoch 154/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.24583\n",
      "Epoch 155/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.24583\n",
      "Epoch 156/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.24583\n",
      "Epoch 157/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.24583\n",
      "Epoch 158/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.24583\n",
      "Epoch 159/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.24583\n",
      "Epoch 160/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.24583\n",
      "Epoch 161/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.24583\n",
      "Epoch 162/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.24583\n",
      "Epoch 163/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.24583\n",
      "Epoch 164/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.24583\n",
      "Epoch 165/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.24583\n",
      "Epoch 166/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.24583\n",
      "Epoch 167/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.24583\n",
      "Epoch 168/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.24583\n",
      "Epoch 169/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.24583\n",
      "Epoch 170/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.24583\n",
      "Epoch 171/250\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.24583\n",
      "Epoch 172/250\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.24583\n",
      "Epoch 173/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.24583\n",
      "Epoch 174/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.24583\n",
      "Epoch 175/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.24583\n",
      "Epoch 176/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.24583\n",
      "Epoch 177/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.24583\n",
      "Epoch 178/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.24583\n",
      "Epoch 179/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.24583\n",
      "Epoch 180/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.24583\n",
      "Epoch 181/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.24583\n",
      "Epoch 182/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.24583\n",
      "Epoch 183/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.24583\n",
      "Epoch 184/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.24583\n",
      "Epoch 185/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.24583\n",
      "Epoch 186/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.24583\n",
      "Epoch 187/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.24583\n",
      "Epoch 188/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.24583\n",
      "Epoch 189/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.24583\n",
      "Epoch 190/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.24583\n",
      "Epoch 191/250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.24583\n",
      "Epoch 192/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.24583\n",
      "Epoch 193/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.24583\n",
      "Epoch 194/250\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.24583\n",
      "Epoch 195/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.24583\n",
      "Epoch 196/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.24583\n",
      "Epoch 197/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.24583\n",
      "Epoch 198/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.24583\n",
      "Epoch 199/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.24583\n",
      "Epoch 200/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.24583\n",
      "Epoch 201/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.24583\n",
      "Epoch 202/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.24583\n",
      "Epoch 203/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.24583\n",
      "Epoch 204/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.24583\n",
      "Epoch 205/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.24583\n",
      "Epoch 206/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.24583\n",
      "Epoch 207/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.24583\n",
      "Epoch 208/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.24583\n",
      "Epoch 209/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.24583\n",
      "Epoch 210/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.24583\n",
      "Epoch 211/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.24583\n",
      "Epoch 212/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.24583\n",
      "Epoch 213/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.24583\n",
      "Epoch 214/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.24583\n",
      "Epoch 215/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.24583\n",
      "Epoch 216/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.24583\n",
      "Epoch 217/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.24583\n",
      "Epoch 218/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.24583\n",
      "Epoch 219/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.24583\n",
      "Epoch 220/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.24583\n",
      "Epoch 221/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.24583\n",
      "Epoch 222/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.24583\n",
      "Epoch 223/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.24583\n",
      "Epoch 224/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.24583\n",
      "Epoch 225/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.24583\n",
      "Epoch 226/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.24583\n",
      "Epoch 227/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.24583\n",
      "Epoch 228/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.24583\n",
      "Epoch 229/250\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.24583\n",
      "Epoch 230/250\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.24583\n",
      "Epoch 231/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.24583\n",
      "Epoch 232/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.24583\n",
      "Epoch 233/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.24583\n",
      "Epoch 234/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.24583\n",
      "Epoch 235/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.24583\n",
      "Epoch 236/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.24583\n",
      "Epoch 237/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.24583\n",
      "Epoch 238/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.24583\n",
      "Epoch 239/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.24583\n",
      "Epoch 240/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.24583\n",
      "Epoch 241/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.24583\n",
      "Epoch 242/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.24583\n",
      "Epoch 243/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2441 - mean_absolute_error: 0.2441 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.24583\n",
      "Epoch 244/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.24583\n",
      "Epoch 245/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.24583\n",
      "Epoch 246/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.24583\n",
      "Epoch 247/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2444 - mean_absolute_error: 0.2444 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.24583\n",
      "Epoch 248/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.24583\n",
      "Epoch 249/250\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.24583\n",
      "Epoch 250/250\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.24583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83b04c8ad0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(X_train,y_train,epochs=250,batch_size=50 ,validation_split = 0.15,callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9ea8ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:59:39.317732Z",
     "iopub.status.busy": "2022-07-17T15:59:39.317039Z",
     "iopub.status.idle": "2022-07-17T15:59:39.327252Z",
     "shell.execute_reply": "2022-07-17T15:59:39.325940Z"
    },
    "papermill": {
     "duration": 0.260595,
     "end_time": "2022-07-17T15:59:39.332537",
     "exception": false,
     "start_time": "2022-07-17T15:59:39.071942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               918016    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,035,010\n",
      "Trainable params: 2,035,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5ef81d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:59:39.818304Z",
     "iopub.status.busy": "2022-07-17T15:59:39.816971Z",
     "iopub.status.idle": "2022-07-17T15:59:40.047791Z",
     "shell.execute_reply": "2022-07-17T15:59:40.046470Z"
    },
    "papermill": {
     "duration": 0.478343,
     "end_time": "2022-07-17T15:59:40.051029",
     "exception": false,
     "start_time": "2022-07-17T15:59:39.572686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(X_test) \n",
    "y_pred_original = mms.inverse_transform(y_pred.reshape(-1,2))\n",
    "y_pred_original = y_pred_original.tolist()\n",
    "x=[i[0] for i in y_pred_original]\n",
    "y=[i[1] for i in y_pred_original]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24fea47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:59:40.531935Z",
     "iopub.status.busy": "2022-07-17T15:59:40.531500Z",
     "iopub.status.idle": "2022-07-17T15:59:40.550455Z",
     "shell.execute_reply": "2022-07-17T15:59:40.549136Z"
    },
    "papermill": {
     "duration": 0.263975,
     "end_time": "2022-07-17T15:59:40.553308",
     "exception": false,
     "start_time": "2022-07-17T15:59:40.289333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"ID\":Id,\"x\":x,\"y\":y})\n",
    "sub1=df.to_csv(\"sub780.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db73fe",
   "metadata": {
    "papermill": {
     "duration": 0.239793,
     "end_time": "2022-07-17T15:59:41.034911",
     "exception": false,
     "start_time": "2022-07-17T15:59:40.795118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1599d65",
   "metadata": {
    "papermill": {
     "duration": 0.245649,
     "end_time": "2022-07-17T15:59:41.578688",
     "exception": false,
     "start_time": "2022-07-17T15:59:41.333039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.957338,
   "end_time": "2022-07-17T15:59:44.879224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-17T15:56:20.921886",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
